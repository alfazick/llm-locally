# llm-locally

# 1. Install and setup
curl https://ollama.ai/install.sh | sh
ollama serve

# 2. Terminal testing
ollama run qwen2.5-coder:0.5b

# 3. Python script
pip install requests

01,02,03,04

# 4. Gradio example 05better_chat.py
pip install gradio 